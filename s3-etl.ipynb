{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ETL with AWS (S3 bucket)\n",
    "\n",
    "[1.Create connection with AWS S3 bucket](#1)\n",
    "\n",
    "[2.Read data from S3 bucket and store into dataframe](#2)\n",
    "\n",
    "[3.Transform data](#3)\n",
    "\n",
    "[4.write data into parquet file](#4) \n",
    "\n",
    "[5.write data into JSON file](#5)\n",
    "\n",
    "## load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ASUS-VIVOBOOK:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>S3-ETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d1fbbbf508>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Spark Session\n",
    "spark = SparkSession.builder. appName(\"S3-ETL\")\\\n",
    "    .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.6.3,org.apache.hadoop:hadoop-common:2.6.3').\\\n",
    "    getOrCreate()\n",
    "\n",
    "sqlContext = SparkSession(spark)\n",
    "\n",
    "#Dont Show warning only error\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 1> </a>\n",
    "\n",
    "## Create Connection with AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set access key and secret key to read and write data from AWS S3\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set('fs.s3a.access.key','Put Access Key Here')\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set('fs.s3a.secret.key','Put Secret Access Key Here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 2> </a>\n",
    "\n",
    "## Read data from S3 bucket and store into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark-etl-s3 is bucket name where data is stored\n",
    "# csv-datasets is folder name\n",
    "# department.csv is filename\n",
    "\n",
    "s3_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load('s3a://pyspark-etl-s3/csv-datasets/employee.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing data type\n",
    "\n",
    "s3_df = s3_df.withColumn('salary', col('salary').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+-------------+\n",
      "| id|first_name|last_name|  salary|department_id|\n",
      "+---+----------+---------+--------+-------------+\n",
      "|  1|      Todd|   Wilson|110000.0|         1006|\n",
      "|  1|      Todd|   Wilson|106119.0|         1006|\n",
      "|  2|    Justin|    Simon|128922.0|         1005|\n",
      "|  2|    Justin|    Simon|130000.0|         1005|\n",
      "|  3|     Kelly|  Rosario| 42689.0|         1002|\n",
      "|  4|  Patricia|   Powell|162825.0|         1004|\n",
      "|  4|  Patricia|   Powell|170000.0|         1004|\n",
      "|  5|    Sherry|   Golden| 44101.0|         1002|\n",
      "|  6|   Natasha|  Swanson| 79632.0|         1005|\n",
      "|  6|   Natasha|  Swanson| 90000.0|         1005|\n",
      "|  7|     Diane|   Gordon| 74591.0|         1002|\n",
      "|  8|  Mercedes|Rodriguez| 61048.0|         1005|\n",
      "|  9|   Christy| Mitchell|137236.0|         1001|\n",
      "|  9|   Christy| Mitchell|140000.0|         1001|\n",
      "|  9|   Christy| Mitchell|150000.0|         1001|\n",
      "| 10|      Sean| Crawford|182065.0|         1006|\n",
      "| 10|      Sean| Crawford|190000.0|         1006|\n",
      "| 11|     Kevin| Townsend|166861.0|         1002|\n",
      "| 12|    Joshua|  Johnson|123082.0|         1004|\n",
      "| 13|     Julie|  Sanchez|185663.0|         1001|\n",
      "+---+----------+---------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 3> </a>\n",
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Register df as SQL Temporary Source View\n"
     ]
    }
   ],
   "source": [
    "print(\"Register df as SQL Temporary Source View\")\n",
    "\n",
    "s3_df.createOrReplaceTempView(\"temp_Source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying top 10 emplpyees with highest salary\n",
      "+----------+---------+--------+\n",
      "|first_name|last_name|  salary|\n",
      "+----------+---------+--------+\n",
      "|     Julie|  Sanchez|210000.0|\n",
      "|     Julie|  Sanchez|200000.0|\n",
      "|   Stephen|    Smith|194791.0|\n",
      "|      Kara|    Smith|192838.0|\n",
      "|      Sean| Crawford|190000.0|\n",
      "|     Linda|    Clark|186781.0|\n",
      "|     Julie|  Sanchez|185663.0|\n",
      "|      Sean| Crawford|182065.0|\n",
      "|   Richard|     Cole|180361.0|\n",
      "|     Traci| Williams|180000.0|\n",
      "+----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Displaying top 10 emplpyees with highest salary\")\n",
    "\n",
    "sqlContext.sql(\"SELECT first_name, last_name, salary from temp_Source order by salary desc limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = sqlContext.sql(\"SELECT first_name, last_name, salary from temp_Source order by salary desc limit 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 4> </a>\n",
    "## write data into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.format(\"parquet\").option(\"compression\",\"snappy\").save(\"parquetdata\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 5> </a>\n",
    "## write data into JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.format(\"json\").option(\"header\",\"true\").save(\"json_data\",mode='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
