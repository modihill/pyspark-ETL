{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spark ETL with Hive tables\n",
    "\n",
    "[1. Read data from Source (MongoDB)](#1)\n",
    "\n",
    "[2. Create and Save Hive table from dataframe](#3)\n",
    "\n",
    "[3. Create temp Hive view from dataframe](#4)\n",
    "\n",
    "[4. Create global Hive view from dataframe](#5)\n",
    "\n",
    "[5. List database and tables in database](#6)\n",
    "\n",
    "[6. Drop all the created tables and views in default database](#7)\n",
    "\n",
    "[7. Create Dataeng database and create global and temp view using SQL](#8) \n",
    "\n",
    "[8. Access global table from other session](#9)\n",
    "\n",
    "\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark session\n",
    "#     config(\"spark.sql.warehouse.dir\",\"warehouse_location\") - will set warehouse location for table to be stored\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Hive Tables\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "    .config(\"spark.sql.warehouse.dir\",\"warehouse_location\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = SparkSession(spark)\n",
    "\n",
    "#showing only error, not warning\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 1>  </a>\n",
    "## Read data from Source (MongoDB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://localhost:27017/\") \\\n",
    "    .option(\"database\", \"dataengineering\") \\\n",
    "    .option(\"collection\", \"employee\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{66dd8e06da00b95e...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 3>  </a>\n",
    "## Create and Save Hive table from dataframe\n",
    "\n",
    "It will save table as parquet file by default in pyspark\\Hive Tables\\spark-warehouse\\hivesampletable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will save table as parquet file by default in pyspark\\Hive Tables\\spark-warehouse\\hivesampletable\n",
    "\n",
    "mongo_df.write.saveAsTable(\"hivesampletable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 4>  </a>\n",
    "## Create temp Hive view from dataframe\n",
    "\n",
    "Temp Hive View tables can only be accessible for particular session in which they are created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_df.createOrReplaceTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 5>  </a>\n",
    "## Create global Hive view from dataframe\n",
    "\n",
    "Global Hive View tables can be accessible for any spark session of that spark application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_df.createOrReplaceGlobalTempView(\"sampleglobalview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 6>  </a>\n",
    "## List database and tables in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/c:/Users/Admin/Desktop/pyspark/Hive Tables/warehouse_location')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show databases\n",
    "\n",
    "sqlContext.sql(\"show databases\").show()\n",
    "\n",
    "\n",
    "# can also be write as\n",
    "\n",
    "sqlContext.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  default|hivesampletable|      false|\n",
      "|         | sampletempview|       true|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='hivesampletable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sampletempview', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show tables\n",
    "\n",
    "sqlContext.sql(\"show tables\").show()\n",
    "\n",
    "\n",
    "\n",
    "# can also be write as\n",
    "\n",
    "sqlContext.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='_id', description=None, dataType='struct<oid:string>', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='department_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get details columns of hive table\n",
    "sqlContext.catalog.listColumns(\"hivesampletable\")\n",
    "\n",
    "\n",
    "# since sampletempview is not in default database and is temporary table, can't get details of columns\n",
    "\n",
    "# sqlContext.catalog.listColumns(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{66dd8e06da00b95e...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM hivesampletable\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{66dd8e06da00b95e...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing dataset of tempView\n",
    "\n",
    "sqlContext.sql(\"SELECT * FROM sampletempview\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{66dd8e06da00b95e...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing data from Global View\n",
    "\n",
    "# since it is global view, need to write global_temp. before table view\n",
    "sqlContext.sql(\"SELECT * FROM global_temp.sampleglobalview\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 7>  </a>\n",
    "## Drop all the created tables and views in default database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView(\"sampleglobalview\")\n",
    "spark.catalog.dropTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 8>  </a>\n",
    "## Create Dataeng database and create global and temp view using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  dataeng|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"create database dataeng\")\n",
    "sqlContext.sql(\"use dataeng\")\n",
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will create and save hivesampletable inside database\n",
    "mongo_df.write.saveAsTable(\"hivesampletable\")\n",
    "\n",
    "\n",
    "mongo_df.createOrReplaceGlobalTempView(\"sampleglobalview\")\n",
    "\n",
    "mongo_df.createOrReplaceTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  dataeng|hivesampletable|      false|\n",
      "|         | sampletempview|       true|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 9>  </a>\n",
    "## Access global table from other session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ASUS-VIVOBOOK:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Hive Tables</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21145d4ec88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Session = spark.newSession()\n",
    "new_Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{66dd8e06da00b95e...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# since it is global view, can be accessed even in new session\n",
    "new_Session.sql(\"SELECT * FROM global_temp.sampleglobalview\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{66dd8e06da00b95e...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{66dd8e06da00b95e...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{66dd8e06da00b95e...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' new_Session.sql(\"Select * from sampletempview\").show(5)\n",
    "\n",
    "this will generate error- because it is temp view created in another session called 'spark'ArithmeticError\n",
    "\n",
    "spark.sql(\"select * from sampletempview\").show(5) -> wil give output\n",
    "'''\n",
    "\n",
    "spark.sql(\"select * from sampletempview\").show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
